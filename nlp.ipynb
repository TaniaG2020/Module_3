{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwEz64DzY8KF",
        "outputId": "5cb5e5ad-9c42-4ea5-fd3e-dab7ee264212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPWqIOaciptG"
      },
      "source": [
        "import tweepy\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# UPDATE: this is the selected company (query for the tweet search)\n",
        "COMPANY = 'apple'\n",
        "\n",
        "def scrape():\n",
        "  '''This function scrapes twitter data which mentions the selected company.\n",
        "  It gets the data for the past three days and returns a dataframe.\n",
        "  INPUT: NONE\n",
        "  OUTPUT: dataframe with text from twitter mentioning the selected company. One row per tweet. '''\n",
        "\n",
        "  access_token = \"1289900179701800960-2lujUQFC8RbuCcOgDMbloRzYPQY7ve\"\n",
        "  access_token_secret = \"7hWUucfMm6oYutWWTBWdCu0RbIIJylfHOM6F0d8CtgH3e\"\n",
        "  consumer_key = \"SJVSihso9VGRMpCC1acdbGk5M\"\n",
        "  consumer_secret = \"TgQLnyhlEwQSIL9tsBVxwW7NNnxWhO5YaYhLN8brlH1WVjiw4x\"\n",
        "\n",
        "\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
        "  \n",
        "  #Dates for scraping tweets: from three days ago until today\n",
        "  startDate=datetime.datetime.today()-datetime.timedelta(days=1)\n",
        "  endDate=datetime.datetime.today()\n",
        "\n",
        "  #Scrape the data\n",
        "  text  =[]\n",
        "  for tweet in tweepy.Cursor(api.search, q=COMPANY, lang=\"en\").items(100):\n",
        "      #keep only the past 3 days\n",
        "      if tweet.created_at <= endDate and tweet.created_at >= startDate:\n",
        "        text.append(tweet.text)\n",
        "\n",
        "  #dump into a panda\n",
        "  text_series = pd.Series(text)\n",
        "\n",
        "  #Perform basica cleaning: remove @mentions, #hastags and URLs\n",
        "\n",
        "  text_series = text_series.str.replace('@[A-Za-z0â€“9]+', '', regex=True)\n",
        "  text_series = text_series.str.replace('#', '', regex=True)\n",
        "  text_series = text_series.str.replace('RT[\\s]+', '', regex=True)\n",
        "\n",
        "  #Return in the form of a dataframe\n",
        "  text_df = pd.DataFrame({'text': text_series})\n",
        "  \n",
        "  return(text_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyB4gySLNDuV"
      },
      "source": [
        "# TEXT CLEANING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "  '''This function does all the necessary preprocessing and cleaning of the texts\n",
        "  so that they can be fed to the nlp for sentiment analysis.\n",
        "  INPUT: a text (string)\n",
        "  OUTPUT:  the cleaned text (string)'''\n",
        "\n",
        "  # Define the stopwords and stemmer\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "  # Remove link,user and special characters\n",
        "  text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    \n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    if token not in stop_words:\n",
        "        if stem:\n",
        "            tokens.append(stemmer.stem(token))\n",
        "        else:\n",
        "            tokens.append(token)\n",
        "  return \" \".join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gkG2VvCYxPc",
        "outputId": "1a506c96-8590-467f-8494-863a8dd3583a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# IMPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\" \n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# KERAS\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "#UTILITY\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "\n",
        "# Downloading the necessary for preprocessing\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# UPDATE: this is the path were the models are stored\n",
        "path = \"/content/drive/Shared drives/NLP sentiment for stocks/model/\"\n",
        "\n",
        "def nlp():\n",
        "  '''This function calls the scraper function which scrapes the past three days of tweets containing the selected company name.\n",
        "  It performs a sentiment analysis of each tweet and gives the average over those three days.\n",
        "  INPUT: NONE\n",
        "  OUTPUT: sentiment average over the past three days.'''\n",
        "\n",
        "  # Loads the saved keras model\n",
        "  def load_models():\n",
        "\n",
        "    # Load the tokenizer.\n",
        "    file = open(path + TOKENIZER_MODEL, 'rb')\n",
        "    tokenizer = pickle.load(file)\n",
        "    file.close()\n",
        "    # Load the model\n",
        "    model = load_model(path + KERAS_MODEL)\n",
        "    # Check its architecture\n",
        "    model.summary()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "  '''\n",
        "  # Gives a label to the sentiment value\n",
        "  def decode_sentiment(score, include_neutral=True):\n",
        "\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "  '''\n",
        "\n",
        "  # Predicts the sentiment value for a tweet\n",
        "  def predict(text, include_neutral=True):\n",
        "\n",
        "    start_at = time.time()\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    #label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    return float(score)\n",
        "\n",
        "  if __name__==\"__main__\":\n",
        "    # Loading the models.\n",
        "    tokenizer, model = load_models()\n",
        "\n",
        "    # Getting the texts\n",
        "    all_text = scrape()\n",
        "\n",
        "    # Preprocessing data\n",
        "    all_text.text = all_text.text.apply(lambda x: preprocess(x))\n",
        "\n",
        "    # Calculating the sum of scores from all tweets over the past three days\n",
        "    score = 0\n",
        "    for text in all_text.text:\n",
        "      score = score + predict(text)\n",
        "\n",
        "    # Calculating the average score for the sentiment over the past three days\n",
        "    score = score/len(all_text)\n",
        "\n",
        "    return(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb95lDqRK471",
        "outputId": "6fbb663c-053e-433b-94cb-d5b0aed54574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(nlp())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 300)          87125700  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 87,286,201\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 87,125,700\n",
            "_________________________________________________________________\n",
            "1\n",
            "1\n",
            "0.874923825263977\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}