{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_sentiment_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwEz64DzY8KF",
        "outputId": "5cb5e5ad-9c42-4ea5-fd3e-dab7ee264212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaSwWvyAcNjM"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import dateutil.parser\n",
        "from urllib.request import urlopen, Request\n",
        "from bs4 import BeautifulSoup\n",
        "import tweepy\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPWqIOaciptG"
      },
      "source": [
        "#Function to get twitter data\n",
        "\n",
        "def twitter_scrape(tags='$AAPL, Apple', from_date=None, until_date=None, today=False):\n",
        "  '''This function scrapes twitter data which mentions the selected company.\n",
        "  It gets the data for the past three days and returns a dataframe.\n",
        "  INPUT: NONE\n",
        "  OUTPUT: dataframe with text from twitter mentioning the selected company. One row per tweet. '''\n",
        "\n",
        "  access_token = \"1289900179701800960-2lujUQFC8RbuCcOgDMbloRzYPQY7ve\"\n",
        "  access_token_secret = \"7hWUucfMm6oYutWWTBWdCu0RbIIJylfHOM6F0d8CtgH3e\"\n",
        "  consumer_key = \"SJVSihso9VGRMpCC1acdbGk5M\"\n",
        "  consumer_secret = \"TgQLnyhlEwQSIL9tsBVxwW7NNnxWhO5YaYhLN8brlH1WVjiw4x\"\n",
        "\n",
        "\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
        "  \n",
        "\n",
        "  #Dates for scraping tweets: from three days ago until today\n",
        "  startDate=datetime.datetime.today()-datetime.timedelta(days=7)\n",
        "  endDate=datetime.datetime.today()\n",
        " \n",
        "  #if today == True:\n",
        "  #   start_date = ddatetime.datetime.today()\n",
        "  #   end_date = datetime.datetime.today()\n",
        "\n",
        "  #Scrape the data\n",
        "  text  =[]\n",
        "  for tweet in tweepy.Cursor(api.search, q=tags, lang=\"en\").items(100):\n",
        "      #keep only the past 3 days\n",
        "      if tweet.created_at <= endDate and tweet.created_at >= startDate:\n",
        "        text.append([tweet.created_at, tweet.text])\n",
        "\n",
        "\n",
        "  dfv = pd.DataFrame(text, columns = ['date', 'text'])\n",
        "  dfv.date = pd.to_datetime(dfv.date)\n",
        "  dfv = dfv.set_index('date')\n",
        "\n",
        "  #filter date\n",
        "  #if today==True:\n",
        "   # day = datetime.date.today()\n",
        "   # mask = (df_full.index == day)\n",
        "   # df_full  = df_full.loc[mask]\n",
        " \n",
        "  #elif from_date != None:\n",
        "   #  start_date = dateutil.parser.parse(from_date).date()\n",
        "    # end_date = dateutil.parser.parse(until_date).date()\n",
        "    # mask = (df_full.index >= start_date) & (df_full.index <= end_date)\n",
        "     #df_full  = df_full.loc[mask]\n",
        "\n",
        "  #Perform basica cleaning: remove @mentions, #hastags and URLs\n",
        "\n",
        "  dfv.text = dfv.text.str.replace('@[A-Za-z0â€“9]+', '', regex=True)\n",
        "  dfv.text = dfv.text.str.replace('#', '', regex=True)\n",
        "  dfv.text = dfv.text.str.replace('RT[\\s]+', '', regex=True)\n",
        "\n",
        "  #Return in the form of a dataframe\n",
        "  #text_df = pd.DataFrame({'text': text_series})\n",
        "  \n",
        "  return(dfv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5oyYzz8TQAA"
      },
      "source": [
        "#function to get news headliens\n",
        "\n",
        "def scrape_news(from_date=None, until_date=None, today=False): \n",
        "  #  To fetch the top headlines\n",
        "  #top_headlines_url = 'https://newsapi.org/v2/top-headlines'\n",
        "  # To fetch news articles\n",
        "  everything_news_url = 'https://newsapi.org/v2/everything'\n",
        "  # To retrieve the sources\n",
        "  #sources_url = 'https://newsapi.org/v2/sources'\n",
        "\n",
        "  # The headers remain the same for all the requests\n",
        "  headers = {'Authorization': '48f69d1519234b22bd0414da9c03d379'}\n",
        "  headlines_payload = {'q': ['apple', 'Apple', '$AAPL'], 'sources': ['bloomberg', 'cnn', 'the-wall-street-journal', 'CNBC', 'Google News', 'MarketWatch', 'Seeking Alpha', 'business-insider'], 'sortBy': 'date'} #'category': 'business', \n",
        "  #sources_payload = {'category': 'general', 'language': 'en', 'country': 'us'}\n",
        "  response = requests.get(url=everything_news_url, headers=headers, params=headlines_payload)\n",
        "  # Convert response to a pure json string\n",
        "  response_json_string = json.dumps(response.json())\n",
        "    # A json object is equivalent to a dictionary in Python\n",
        "    # retrieve json objects to a python dict\n",
        "  response_dict = json.loads(response_json_string)\n",
        "  #print(response_dict)\n",
        " # Info about articles is represented as an array in the json response\n",
        "  # A json array is equivalent to a list in python\n",
        "  # We want info only about articles\n",
        "  news_list = response_dict['articles']\n",
        "  df = pd.DataFrame(news_list)\n",
        "  dfnew = df[['publishedAt', 'title', 'description']]\n",
        "  dfnew['date'] = pd.to_datetime(dfnew['publishedAt'])\n",
        "  dfnew['date'] = dfnew.date.dt.date\n",
        "  df = dfnew[['date', 'title']]\n",
        "  df = df.rename(columns={'title': 'text'})\n",
        "  df = df.set_index('date')\n",
        "\n",
        "  #FINVIZ\n",
        "  finwiz_url = 'https://finviz.com/quote.ashx?t=AAPL'\n",
        "\n",
        "  news_tables = {}\n",
        "  req = Request(url=finwiz_url,headers={'user-agent': 'my-app/0.0.1'})\n",
        "  response = urlopen(req)\n",
        "  # Read the contents of the file into 'html'\n",
        "  html = BeautifulSoup(response, features='lxml')\n",
        "  # Find 'news-table' in the Soup and load it into 'news_table'\n",
        "  news_table = html.find(id='news-table')\n",
        "  parsed_news = []\n",
        "\n",
        "  # Iterate through the news\n",
        "  for x in news_table.findAll('tr'):\n",
        "        # read the text from each tr tag into text\n",
        "        # get text from a only\n",
        "        text = x.a.get_text()\n",
        "        # splite text in the td tag into a list\n",
        "        date_scrape = x.td.text.split()\n",
        "        # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
        "\n",
        "        if len(date_scrape) == 1:\n",
        "            time = date_scrape[0]\n",
        "\n",
        "        # else load 'date' as the 1st element and 'time' as the second\n",
        "        else:\n",
        "            date = date_scrape[0]\n",
        "            time = date_scrape[1]\n",
        "        \n",
        "        parsed_news.append([date, text])\n",
        "        \n",
        "  dfv = pd.DataFrame(parsed_news, columns = ['date', 'text'])\n",
        "  dfv.date = pd.to_datetime(dfv.date)\n",
        "  dfv = dfv.set_index('date')\n",
        "\n",
        "  df_full = pd.concat([df, dfv])\n",
        "\n",
        "  #filter date\n",
        "  if today==True:\n",
        "    day = datetime.date.today()\n",
        "    mask = (df_full.index == day)\n",
        "    df_full  = df_full.loc[mask]\n",
        " \n",
        "  elif from_date != None:\n",
        "     start_date = dateutil.parser.parse(from_date).date()\n",
        "     end_date = dateutil.parser.parse(until_date).date()\n",
        "     mask = (df_full.index >= start_date) & (df_full.index <= end_date)\n",
        "     df_full  = df_full.loc[mask]\n",
        "\n",
        "  df_full.text = df_full.text.str.replace('[#@]', '', regex=True)\n",
        "  df_full.text = df_full.text.str.replace('RT[\\s]+', '', regex=True)\n",
        "\n",
        "  return df_full "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ipHRSkGXPSR"
      },
      "source": [
        "#function to get all data \n",
        "\n",
        "def getdata(twitter_scrape=twitterscrape, scrape_news=scrape_news):\n",
        "  df1 = twitter_scrape()\n",
        "  df2 = scrape_news()\n",
        "  df_full = pd.concat([df1, df2])\n",
        "  return df_full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyB4gySLNDuV"
      },
      "source": [
        "# TEXT CLEANING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "  '''This function does all the necessary preprocessing and cleaning of the texts\n",
        "  so that they can be fed to the nlp for sentiment analysis.\n",
        "  INPUT: a text (string)\n",
        "  OUTPUT:  the cleaned text (string)'''\n",
        "\n",
        "  # Define the stopwords and stemmer\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "  # Remove link,user and special characters\n",
        "  text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    \n",
        "  tokens = []\n",
        "  for token in text.split():\n",
        "    if token not in stop_words:\n",
        "        if stem:\n",
        "            tokens.append(stemmer.stem(token))\n",
        "        else:\n",
        "            tokens.append(token)\n",
        "  return \" \".join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gkG2VvCYxPc",
        "outputId": "1a506c96-8590-467f-8494-863a8dd3583a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# IMPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\" \n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# KERAS\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "#UTILITY\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "\n",
        "# Downloading the necessary for preprocessing\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# UPDATE: this is the path were the models are stored\n",
        "path = \"/content/drive/Shared drives/NLP sentiment for stocks/model/\"\n",
        "\n",
        "def nlp():\n",
        "  '''This function calls the scraper function which scrapes the past three days of tweets containing the selected company name.\n",
        "  It performs a sentiment analysis of each tweet and gives the average over those three days.\n",
        "  INPUT: NONE\n",
        "  OUTPUT: sentiment average over the past three days.'''\n",
        "\n",
        "  # Loads the saved keras model\n",
        "  def load_models():\n",
        "\n",
        "    # Load the tokenizer.\n",
        "    file = open(path + TOKENIZER_MODEL, 'rb')\n",
        "    tokenizer = pickle.load(file)\n",
        "    file.close()\n",
        "    # Load the model\n",
        "    model = load_model(path + KERAS_MODEL)\n",
        "    # Check its architecture\n",
        "    model.summary()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "  '''\n",
        "  # Gives a label to the sentiment value\n",
        "  def decode_sentiment(score, include_neutral=True):\n",
        "\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE\n",
        "  '''\n",
        "\n",
        "  # Predicts the sentiment value for a tweet\n",
        "  def predict(text, include_neutral=True):\n",
        "\n",
        "    start_at = time.time()\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    #label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    return float(score)\n",
        "\n",
        "  if __name__==\"__main__\":\n",
        "    # Loading the models.\n",
        "    tokenizer, model = load_models()\n",
        "\n",
        "    # Getting the texts\n",
        "    all_text = scrape()\n",
        "\n",
        "    # Preprocessing data\n",
        "    all_text.text = all_text.text.apply(lambda x: preprocess(x))\n",
        "\n",
        "    # Calculating the sum of scores from all tweets over the past three days\n",
        "    score = 0\n",
        "    for text in all_text.text:\n",
        "      score = score + predict(text)\n",
        "\n",
        "    # Calculating the average score for the sentiment over the past three days\n",
        "    score = score/len(all_text)\n",
        "\n",
        "    return(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb95lDqRK471",
        "outputId": "6fbb663c-053e-433b-94cb-d5b0aed54574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(nlp())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 300)          87125700  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 300, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 87,286,201\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 87,125,700\n",
            "_________________________________________________________________\n",
            "1\n",
            "1\n",
            "0.874923825263977\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}